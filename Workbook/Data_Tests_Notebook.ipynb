{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Big Data - Deequ Analysis\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "## Honour Code\n",
    "I **HAKIM BALOGUN**, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "    Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "Having completed manual data quality checks, it should be obvious that the process can become quite cumbersome. As the Data Engineer in the team, you have researched some tools that could potentially save the team from having to do this cumbersome work. In your research, you have come a across a tool called [Deequ](https://github.com/awslabs/deequ), which is a library for measuring the data quality of large datasets.\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
    "     alt=\"Data Quality\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=100%/>\n",
    "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
    "</div>\n",
    "\n",
    "You present this tool to your manager; he is quite impressed and gives you the go-ahead to use this in your implementation. You are now required to perform some data quality tests using this automated data testing tool.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 🚩️ Important Notice 🚩️\n",
    ">\n",
    ">To successfully run `pydeequ` without any errors, please make sure that you have an environment that is running pyspark version 3.0.\n",
    "> You are advised to **create a new conda environment** and install this specific version of pyspark to avoid any technical issues:\n",
    ">\n",
    "> `pip install pyspark==3.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "## Import dependencies\n",
    "\n",
    "If you do not have `pydeequ` already installed, install it using the following command:\n",
    "- `pip install pydeequ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydeequ\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.profiles import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into spark dataframe\n",
    "\n",
    "In this notebook, we set out to run some data quality tests, with the possiblity of running end to end on the years 1963, 1974, 1985, 1996, 2007, and 2018. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Data_ingestion_student_version.ipynb` notebook to create the parquet files for the following years:\n",
    ">       - 1963\n",
    ">       - 1974\n",
    ">       - 1985\n",
    ">       - 1996\n",
    ">       - 2007\n",
    ">       - 2018\n",
    ">\n",
    ">2. Ingest the data for the for the years given above. You should only do it one year at a time.\n",
    ">3. Ingest the metadata file.\n",
    "\n",
    "\n",
    "When developing your code, it will be sufficient to focus on a single year. However, after your development is done, you will need to run this notebook for all of the given years above so that you can answer all the questions given in the Data Testing MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+---------+----------+----------+--------+-----+\n",
      "|               date|      open|      high|      low|     close| adj_close|  volume|stock|\n",
      "+-------------------+----------+----------+---------+----------+----------+--------+-----+\n",
      "|1963-10-28 00:00:00|  6.924645|  6.940665| 6.908625|  6.908625| 1.6844347| 39900.0|   AA|\n",
      "|1963-10-28 00:00:00|  6.488943| 6.5123687| 6.465517|  6.465517| 1.5472991| 42600.0| ARNC|\n",
      "|1963-10-28 00:00:00|0.56790125|0.56995887|0.5617284|0.56995887|0.11004562|789600.0|   BA|\n",
      "+-------------------+----------+----------+---------+----------+----------+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here\n",
    "# Use this variable (year) to determine which year your are focusing on\n",
    "year_1 = 1963 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_1963.parquet'\n",
    "df_63 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_63.show(3) \n",
    "\n",
    "df_63.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+--------+-------+---------+--------+-----+\n",
      "|               date|     open|     high|     low|  close|adj_close|  volume|stock|\n",
      "+-------------------+---------+---------+--------+-------+---------+--------+-----+\n",
      "|1974-11-06 00:00:00|4.9802175|5.0402923|4.818015|4.83003|1.5678892|173000.0|   AA|\n",
      "|1974-11-06 00:00:00|      0.0|    7.875|   7.625|  7.625| 5.073933| 25400.0|  AEM|\n",
      "|1974-11-06 00:00:00|    16.25|   16.625|  16.125| 16.375|0.6937138| 48600.0|  AEP|\n",
      "+-------------------+---------+---------+--------+-------+---------+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_2 = 1974 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_1974.parquet'\n",
    "df_74 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_74.show(3) \n",
    "\n",
    "df_74.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+---------+---------+-------------+--------+-----+\n",
      "|               date|     open|     high|      low|    close|    adj_close|  volume|stock|\n",
      "+-------------------+---------+---------+---------+---------+-------------+--------+-----+\n",
      "|1985-12-19 00:00:00|11.858805|   12.015|11.834775|11.834775|     6.174869|856500.0|   AA|\n",
      "|1985-12-19 00:00:00|      0.0|     12.4|     12.2|     12.4|   10.7758875|  4000.0| AAME|\n",
      "|1985-12-19 00:00:00|1.2037038|1.2037038|1.1666666|1.2037038|-1.5022586E21|665500.0|  AAN|\n",
      "+-------------------+---------+---------+---------+---------+-------------+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_3 = 1985 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_1985.parquet'\n",
    "df_85 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_85.show(3) \n",
    "\n",
    "df_85.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------+---------+---------+---------+---------+-----+\n",
      "|               date|     open|    high|      low|    close|adj_close|   volume|stock|\n",
      "+-------------------+---------+--------+---------+---------+---------+---------+-----+\n",
      "|1996-05-21 00:00:00| 37.94337|38.64024| 37.84725| 38.08755|26.837284|1635400.0|   AA|\n",
      "|1996-05-21 00:00:00|     3.75|    3.75|      3.5|    3.625| 3.349904|  15200.0| AAME|\n",
      "|1996-05-21 00:00:00|4.4722223|4.537037|4.4444447|4.4444447|-156.1936|  23500.0|  AAN|\n",
      "+-------------------+---------+--------+---------+---------+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_4 = 1996 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_1996.parquet'\n",
    "df_96 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_96.show(3) \n",
    "\n",
    "df_96.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---------+---------+---------+---------+---------+-----+\n",
      "|               date|    open|     high|      low|    close|adj_close|   volume|stock|\n",
      "+-------------------+--------+---------+---------+---------+---------+---------+-----+\n",
      "|2007-12-27 00:00:00|26.67382|26.845493|26.566525|26.566525|24.249578|2628500.0|    A|\n",
      "|2007-12-27 00:00:00|89.70399| 89.77608| 88.83891| 88.88697| 77.58712|2743600.0|   AA|\n",
      "|2007-12-27 00:00:00|   15.43|    15.62|    15.08|    15.18|14.310814|1473200.0|  AAL|\n",
      "+-------------------+--------+---------+---------+---------+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_5 = 2007 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_2007.parquet'\n",
    "df_07 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_07.show(3) \n",
    "\n",
    "df_07.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+-----+-----+---------+---------+-----+\n",
      "|               date| open| high|  low|close|adj_close|   volume|stock|\n",
      "+-------------------+-----+-----+-----+-----+---------+---------+-----+\n",
      "|2018-12-18 00:00:00|68.45|69.03|67.39|67.99| 67.07753|2112000.0|    A|\n",
      "|2018-12-18 00:00:00|28.12|28.66|28.09|28.24|    28.24|2107800.0|   AA|\n",
      "|2018-12-18 00:00:00|32.47|33.65|32.47|33.54| 33.00629|9117600.0|  AAL|\n",
      "+-------------------+-----+-----+-----+-----+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_6 = 2018 \n",
    "parquet_file_path = r'C:\\Tests\\Predict\\output\\data_ingestion_2018.parquet'\n",
    "df_18 = spark.read.parquet(parquet_file_path, engine='auto')\n",
    "\n",
    "df_18.show(3) \n",
    "\n",
    "df_18.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "|Nasdaq Traded|Symbol|       Security Name|Listing Exchange|Market Category|ETF|Round Lot Size|Test Issue|Financial Status|CQS Symbol|NASDAQ Symbol|NextShares|\n",
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "|            Y|     A|Agilent Technolog...|               N|               |  N|         100.0|         N|            null|         A|            A|         N|\n",
      "|            Y|    AA|Alcoa Corporation...|               N|               |  N|         100.0|         N|            null|        AA|           AA|         N|\n",
      "|            Y|  AAAU|Perth Mint Physic...|               P|               |  Y|         100.0|         N|            null|      AAAU|         AAAU|         N|\n",
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- Nasdaq Traded: string (nullable = true)\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- Security Name: string (nullable = true)\n",
      " |-- Listing Exchange: string (nullable = true)\n",
      " |-- Market Category: string (nullable = true)\n",
      " |-- ETF: string (nullable = true)\n",
      " |-- Round Lot Size: string (nullable = true)\n",
      " |-- Test Issue: string (nullable = true)\n",
      " |-- Financial Status: string (nullable = true)\n",
      " |-- CQS Symbol: string (nullable = true)\n",
      " |-- NASDAQ Symbol: string (nullable = true)\n",
      " |-- NextShares: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta = spark.read.csv('C:\\Tests\\Predict\\data\\symbols_valid_meta.csv', header=True) \n",
    "\n",
    "meta.show(3) \n",
    "\n",
    "meta.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1963, 1974, 1985, 1996, 2007, 2018]\n"
     ]
    }
   ],
   "source": [
    "list_of_years = [year_1, year_2, year_3, year_4, year_5, year_6] \n",
    "\n",
    "list_of_dfs = [df_63, df_74, df_85, df_96, df_07, df_18] \n",
    "\n",
    "print(list_of_years) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run tests on the dataset**\n",
    "\n",
    "## Test 1 - Null values ⛔️\n",
    "For the first test, you are required to check the data for completeness.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for missing values in the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Null Checks|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "+-----------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|      check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Null Checks|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "+-----------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs: \n",
    "    check = Check(spark, CheckLevel.Warning, \"Null Checks\") \n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.isComplete(\"date\") \\\n",
    "            .isComplete(\"open\") \\\n",
    "            .isComplete(\"high\")  \\\n",
    "            .isComplete(\"low\") \\\n",
    "            .isComplete(\"close\") \\\n",
    "            .isComplete(\"adj_close\")\\\n",
    "            .isComplete(\"volume\")\\\n",
    "            .isComplete(\"stock\")) \\\n",
    "        .run()\n",
    "\n",
    "    checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    checkResult_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Zero Values 🅾️\n",
    "\n",
    "For the second test, you are required to check for zero values within the dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for zero values within the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.54980079...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.52619174...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.51907134...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.00169367...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                  |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                  |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                  |\n",
      "+----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 7.62682261...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 7.62682261...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs: \n",
    "    check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.satisfies(\"open==0\",\"Checking for zeros\", lambda x:x==0) \\\n",
    "            .satisfies(\"high==0\",\"Checking for zeros\", lambda x:x==0)  \\\n",
    "            .satisfies(\"low==0\",\"Checking for zeros\", lambda x:x==0) \\\n",
    "            .satisfies(\"close==0\",\"Checking for zeros\", lambda x:x==0) \\\n",
    "            .satisfies(\"adj_close==0\",\"Checking for zeros\", lambda x:x==0)\\\n",
    "            .satisfies(\"volume==0\",\"Checking for zeros\", lambda x:x==0)) \\\n",
    "        .run()\n",
    "\n",
    "    checkResult_df_zero = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    checkResult_df_zero.show(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Negative values ➖️\n",
    "The third test requires you to check that all values in the data are positive.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check negative values within the dataset. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99437477...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99515556...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99907791...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99936366...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs: \n",
    "    check = Check(spark, CheckLevel.Warning, \"Review Check\") \n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.isNonNegative('open') \\\n",
    "            .isNonNegative('high') \\\n",
    "            .isNonNegative(\"low\")  \\\n",
    "            .isNonNegative(\"close\")  \\\n",
    "            .isNonNegative(\"adj_close\") \\\n",
    "            .isNonNegative(\"volume\")) \\\n",
    "        .run()\n",
    "\n",
    "    checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    checkResult_df.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 - Determine Maximum Values ⚠️\n",
    "\n",
    "For the fourth test, we want to find the maximum values in the dataset for the numerical fields. Extremum values can often be used to define an upper bound for the column values so we can define them as the threshold values. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Column Profiler Runner` to generate summary statistics for all the available columns. \n",
    ">2. Extract the maximum values for all the numeric columns in the data.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1144.run.\n: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'\r\n\tat org.apache.spark.sql.DeequFunctions$.withAggregateFunction(DeequFunctions.scala:31)\r\n\tat org.apache.spark.sql.DeequFunctions$.stateful_approx_count_distinct(DeequFunctions.scala:60)\r\n\tat com.amazon.deequ.analyzers.ApproxCountDistinct.aggregationFunctions(ApproxCountDistinct.scala:52)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.$anonfun$runScanningAnalyzers$3(AnalysisRunner.scala:319)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:366)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:319)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)\r\n\tat com.amazon.deequ.profiles.ColumnProfiler$.profile(ColumnProfiler.scala:141)\r\n\tat com.amazon.deequ.profiles.ColumnProfilerRunner.run(ColumnProfilerRunner.scala:72)\r\n\tat com.amazon.deequ.profiles.ColumnProfilerRunBuilder.run(ColumnProfilerRunBuilder.scala:185)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Tests\\Predict\\de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Tests/Predict/de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m#TODO: Write your code here \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Tests/Predict/de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb#ch0000013?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m list_of_dfs: \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Tests/Predict/de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb#ch0000013?line=2'>3</a>\u001b[0m     result \u001b[39m=\u001b[39m ColumnProfilerRunner(spark)\u001b[39m.\u001b[39;49monData(df)\u001b[39m.\u001b[39;49mrun() \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Tests/Predict/de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb#ch0000013?line=3'>4</a>\u001b[0m     num_cols \u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madj_close\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Tests/Predict/de-pbd-hakim_balogun-Data_Deequ_Tests-notebook.ipynb#ch0000013?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m num_cols:\n",
      "File \u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pydeequ\\profiles.py:121\u001b[0m, in \u001b[0;36mColumnProfilerRunBuilder.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    116\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    A method that runs a profile check on the data to obtain a ColumnProfiles class\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[39m    :return: A ColumnProfiles result\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ColumnProfilerRunBuilder\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m ColumnProfilesBuilder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark_session)\u001b[39m.\u001b[39m_columnProfilesFromColumnRunBuilderRun(run)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1144.run.\n: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'\r\n\tat org.apache.spark.sql.DeequFunctions$.withAggregateFunction(DeequFunctions.scala:31)\r\n\tat org.apache.spark.sql.DeequFunctions$.stateful_approx_count_distinct(DeequFunctions.scala:60)\r\n\tat com.amazon.deequ.analyzers.ApproxCountDistinct.aggregationFunctions(ApproxCountDistinct.scala:52)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.$anonfun$runScanningAnalyzers$3(AnalysisRunner.scala:319)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:366)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:319)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)\r\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)\r\n\tat com.amazon.deequ.profiles.ColumnProfiler$.profile(ColumnProfiler.scala:141)\r\n\tat com.amazon.deequ.profiles.ColumnProfilerRunner.run(ColumnProfilerRunner.scala:72)\r\n\tat com.amazon.deequ.profiles.ColumnProfilerRunBuilder.run(ColumnProfilerRunBuilder.scala:185)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs: \n",
    "    result = ColumnProfilerRunner(spark).onData(df).run() \n",
    "    num_cols =['open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "    for col in num_cols:\n",
    "        col_profile = result.profiles[col]\n",
    "        print(f'Statistics of {col}:') \n",
    "        print('\\t',f\"minimum: {col_profile.minimum.get}\")\n",
    "        print('\\t',f\"maximum: {col_profile.maximum.get}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 - Stock Tickers 💹️\n",
    "\n",
    "For the fifth test, we want to determine if the stock tickers contained in our dataset are consistent. To do this, you will need to make use of use of the metadata file to check that the stock names used in the dataframe are valid. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine if the stock tickers contained in the dataset appear in the metadata file.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Stock ticker cons...|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Stock ticker cons...|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99437477...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Stock ticker cons...|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99854320...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Stock ticker cons...|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99878647...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Stock ticker cons...|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99935598...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Stock ticker cons...|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99958662...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs: \n",
    "    list(set(df.select('stock').rdd.flatMap(lambda x: x).collect())-set(meta.select('symbol').rdd.flatMap(lambda x: x).collect())) \n",
    "\n",
    "    check = Check(spark, CheckLevel.Warning, \"Stock ticker consistency Check\") \n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(  \n",
    "        check.isContainedIn(\"stock\",np.array(meta.select('Symbol').collect()).reshape(-1))\\\n",
    "        )\\\n",
    "        .run()\n",
    "\n",
    "    checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    checkResult_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6 - Duplication 👥️\n",
    "Lastly, we want to determine the uniqueness of the items found in the dataframe. You need to make use of the Verification Suite to check for the validity of the stock tickers. \n",
    "\n",
    "Similar to the previous notebook - `Data_profiling_student_version.ipynb`, the first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date. Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine the uniqueness of entries contained within the dataset.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Unique Check|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here \n",
    "for df in list_of_dfs:\n",
    "    check = Check(spark, CheckLevel.Error, \"Unique Check\")\n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.hasUniqueness([\"date\",\"stock\"], lambda x: x == 1)) \\\n",
    "        .run()\n",
    "\n",
    "    checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    checkResult_df.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
